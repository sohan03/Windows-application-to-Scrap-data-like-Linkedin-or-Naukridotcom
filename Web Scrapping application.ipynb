{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d90a7236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tk in c:\\users\\sohan\\anaconda3\\lib\\site-packages (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4853f332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sohan\\anaconda3\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-2-38827f888da8>\", line 69, in scrap\n",
      "    ExtractSectionUrl()\n",
      "  File \"<ipython-input-2-38827f888da8>\", line 43, in ExtractSectionUrl\n",
      "    SectionTags = MyParser.find(\"section\",{\"class\":\"explore-section\"}).find_all('a')\n",
      "AttributeError: 'NoneType' object has no attribute 'find_all'\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import Label, ttk, messagebox\n",
    "from tkinter import font\n",
    "from tkinter.constants import DISABLED, NORMAL\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "#from requests.api import post\n",
    "#from bs4.element import Tag\n",
    "\n",
    "# create a window window\n",
    "window = tk.Tk()\n",
    " \n",
    "# set the title of window window\n",
    "window.title(\"Web Scrapper\")\n",
    "\n",
    " \n",
    "# set the configuration of window window\n",
    "window.geometry(\"400x200\")\n",
    "#window.resizable(0, 0)\n",
    "\n",
    "# get the website from the user\n",
    "def get_website():\n",
    "    def linkedin():\n",
    "        button1.destroy()\n",
    "        button2.destroy()\n",
    "        \n",
    "\n",
    "        def back_to_website():\n",
    "            click_label.destroy()\n",
    "            scrap_button.destroy()\n",
    "            download_button.destroy()\n",
    "            back_button.destroy()\n",
    "            get_website()\n",
    "\n",
    "        def ExtractSectionUrl():\n",
    "            the_url= \"https://www.linkedin.com/content-hub/?trk=homepage-basic_guest_nav_menu_discover\"\n",
    "\n",
    "            data = requests.get(the_url).content #get the contents of the page using get method\n",
    "            MyParser = BeautifulSoup(data,'lxml')\n",
    "\n",
    "            SectionTags = MyParser.find(\"section\",{\"class\":\"explore-section\"}).find_all('a')\n",
    "\n",
    "            SectionLinks=[]#find Section URLS and store in list\n",
    "            for links in SectionTags:\n",
    "                SectionLinks.append(links.get('href'))#append every link in the list\n",
    "                print(\"getting section URLS...\")\n",
    "            ExtractPostURL(SectionLinks) #function call in order to pass the Section Links to the ExtractPostURL funtion\n",
    "\n",
    "        def ExtractPostURL(SectionLinks):\n",
    "            global ProfileLinks\n",
    "            SectionURL = SectionLinks\n",
    "            ProfileLinks = [] #to store the links of the profiles\n",
    "            for link in SectionURL:#get the contents of every urls\n",
    "                pagedata = requests.get(link).content\n",
    "                MySoup = BeautifulSoup(pagedata,'lxml')\n",
    "\n",
    "                Posttag = MySoup.find_all(\"h3\",{\"class\":\"share-update-card__actor-text\"})\n",
    "                for n in range(len(Posttag)):\n",
    "                    atag = Posttag[n].find(\"a\")\n",
    "                    print(\"extracting profiles... \\nplease wait for some seconds\")\n",
    "                    ProfileLinks.append(atag.get(\"href\"))\n",
    "                    n=n+1\n",
    "\n",
    "        def scrap():\n",
    "            click_label.destroy()\n",
    "            scrap_button.config(state=DISABLED)\n",
    "            ExtractSectionUrl()\n",
    "            time.sleep(15)\n",
    "            download_button.config(state=NORMAL)\n",
    "            file_ready_label = Label(window, text=\"File Ready!\", font=('Segoe UI',17, font.BOLD))\n",
    "            file_ready_label.place(x=110,y=40)\n",
    "\n",
    "        def download():\n",
    "            \n",
    "            with open(\"profilelinks.csv\",'w') as f:\n",
    "                f.write(str(ProfileLinks))\n",
    "                f.close\n",
    "\n",
    "            messagebox.showinfo(\"info\", \"File downloaded successfully!\")\n",
    "            \n",
    "\n",
    "        click_label = Label(window, text=\"Click on Scrap Data\\n to start scraping\", font=('Segoe UI',17, font.BOLD))\n",
    "        click_label.place(x=90,y=40)\n",
    "\n",
    "        scrap_button = ttk.Button(window, text=\"Scrap Data\", command=scrap, width=10)\n",
    "        scrap_button.place(x=115,y=140)\n",
    "        scrap_button.focus_set()\n",
    "\n",
    "        back_button = ttk.Button(window, text=\"Back\", command=back_to_website, width=10)\n",
    "        back_button.place(x=30,y=140)\n",
    "        download_button = ttk.Button(window, text=\"Download\", command=download, width=10, state=DISABLED)\n",
    "        download_button.place(x=200,y=140)\n",
    "        \n",
    "    def naukri():\n",
    "        button1.destroy()\n",
    "        button2.destroy()\n",
    "        \n",
    "        def back_to_website():\n",
    "            click_label.destroy()\n",
    "            scrap_button.destroy()\n",
    "            download_button.destroy()\n",
    "            back_button.destroy()\n",
    "            get_website()\n",
    "\n",
    "        def extract_profile():\n",
    "            global profile_data, urls\n",
    "            url='https://www.naukri.com/python-developer-recruiters-in-bangalore'\n",
    "            r=requests.get(url) #get - extracts the content from the website url\n",
    "\n",
    "            urls=list() #list of all profile urls\n",
    "            soup = BeautifulSoup(r.text,'html.parser') #parsing website main page\n",
    "\n",
    "            for a in soup.find_all('a',{'class':'ellipsis','target':'_blank'}):\n",
    "                urls.append(a['href']) #adds all url of people into a list\n",
    "                print(\"getting URLS of people...\")\n",
    "            #----- main page parsing is over------\n",
    "            #----- individual page parsing starts----\n",
    "                \n",
    "            profile_data = list()\n",
    "            for url in urls:\n",
    "                profile=dict()\n",
    "                print(\"getting datas...\")\n",
    "                page_soup=BeautifulSoup(requests.get(url).text,'html.parser') #parsing individual profile page\n",
    "                name=page_soup.find('h1',{'class':'fl ellipsis wLimit hd'})\n",
    "                profile['name']=name.text #getting name of individual\n",
    "                role=page_soup.find('div',{'class':'ellipsis'})\n",
    "                profile['role']=role.text #role of individual\n",
    "                Company_Name=page_soup.find('a',{'class':'fl ellipsis widLrg'})\n",
    "                profile['company_name']=Company_Name.text #company of individual\n",
    "                profile['location']='Bangalore'\n",
    "                profile_data.append(profile) #adding name,role,company,location to list\n",
    "\n",
    "        def scrap():\n",
    "            click_label.destroy()\n",
    "            scrap_button.config(state=DISABLED)\n",
    "            scraping_label = Label(window, text=\"Scrapping data...\", font=('Segoe UI',17, font.BOLD))\n",
    "            scraping_label.place(x=70,y=20)\n",
    "            extract_profile()\n",
    "            time.sleep(15)\n",
    "            download_button.config(state=NORMAL)\n",
    "            scraping_label.destroy()\n",
    "            \n",
    "            file_naukri = Label(window, text=\"File Ready!\", font=('Segoe UI',17, font.BOLD))\n",
    "            file_naukri.place(x=110,y=40)\n",
    "\n",
    "        def download():\n",
    "\n",
    "            profiles = pd.DataFrame(profile_data) #converting list into a dataframe( similar to excel)\n",
    "            urls_df=pd.DataFrame(urls,columns=['profile_url']) #column name for urls\n",
    "            naukri_scrape=pd.concat([profiles,urls_df],axis=1)\n",
    "            naukri_scrape.to_csv('naukri_scrape.csv')\n",
    "\n",
    "            messagebox.showinfo(\"info\", \"File downloaded successfully!\")\n",
    "            \n",
    "\n",
    "        click_label = Label(window, text=\"Click on Scrap Data\\n to start scraping\", font=('Segoe UI',17, font.BOLD))\n",
    "        click_label.place(x=90,y=40)\n",
    "\n",
    "        scrap_button = ttk.Button(window, text=\"Scrap Data\", command=scrap, width=10)\n",
    "        scrap_button.place(x=115,y=140)\n",
    "        scrap_button.focus_set()\n",
    "\n",
    "        back_button = ttk.Button(window, text=\"Back\", command=back_to_website, width=10)\n",
    "        back_button.place(x=30,y=140)\n",
    "        download_button = ttk.Button(window, text=\"Download\", command=download, width=10, state=DISABLED)\n",
    "        download_button.place(x=200,y=140)\n",
    "\n",
    "    button1 = tk.Button(window, text=\"LinkedIn\", command=linkedin, width=10, font=('Segoe UI',20))\n",
    "    button2 = tk.Button(window, text=\"Naukri.com\", command=naukri, width=10, font=('Segoe UI',20))\n",
    "    button1.place(x=80,y=20)\n",
    "    button2.place(x=80,y=100)\n",
    "\n",
    "    quit_button = ttk.Button(window, text=\"Quit\", command=quit, width=10)\n",
    "    quit_button.place(x=285, y=140)\n",
    "\n",
    "get_website()\n",
    "\n",
    "\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fef1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605b159c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
